<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>A Few BeautifulSoup Examples</title>
    
    <link href="/css/common.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  </head>
  <body>
    
    <section class = "banner">
    <h1>A Few BeautifulSoup Examples</h1>
    <h2>22 May 2020</h2>
    </section>
	<html>
<head>
<style>
.Navigation{
  list-style-type: none;
  margin: 0;
  padding: 0;
  overflow: hidden;
}

.Navigation li{
  float: right;
}

.Navigation li a{
  display: block;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
}

.Navigation > :first-child {
  float: left;
} 

li a:hover:not(.active) {
  background-color: #c6d1c8;
}

.active {
  background-color: #c6d1c8;
}
</style>
</head>

<div class="navbar">
  <ul class = "Navigation">
  
    <li><a href="/blog.html" ><- Blog</a></li>
  
  </ul>
</div>
</html>

    <div class = "narration">
      <h2>Let's have a go at it</h2>

<p>There is so much data available on the internet. I'm sure you already know that. The results of any modern sports tournament are hosted somewhere. Countless blog posts and movie reviews are available to the public free of charge. Often, the questions that I want answered need a dataset that hasn't been built yet. No problem. I've seen what a dataset looks like, I can certainly build one myself.</p>

<p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> is a Python library that makes it a snap to request HTML pages and parse the information on them. With a few short loops web pages are transformed into a luxurious CSV or JSON file. BeautifulSoup is web scraper as opposed to a web crawler. Web crawlers can be programmed to traverse pages on their own when they encounter a link while web scrapers must be told which pages to parse.</p>

<p>Scraping a set of web pages has 2 parts: how do I extract the information on an individual page and how do I get the set of pages to parse from? It comes down to pattern recognition and using the element inspector. The user score could have its own div. The URLs of each page may only differ by a date or city name. Better yet there could be a page that contains the URL for every episode. Each situation is going to be different so let's take a look at some examples.</p>

<h2>Collecting a list of National Parks</h2>

<p>In this example I start with the individual page for Florida's National Parks. The code below requests the page html and prints the names and links of the parks. I manually found the div that contains the park information and manually chose to search for the h3 tags because thats how the page chose to display the names. </p>

<pre>
  <code>
    from bs4 import BeautifulSoup
    import requests

    url = "https://www.nps.gov/state/fl/index.htm"
    soup = BeautifulSoup(requests.get(url).content)

    florida = soup.find(id = "parkListResultsArea")
    parks = florida.find_all('h3')

    for park in parks:
        link = park.find('a')
        print(link)
  </code>
</pre>
     
<a href="https://github.com/jchakko/D3_Manhattan">Link's up!</a>
    </div>
	
  </body>
</html>